"""
Whisper-tiny ë°°ì¹˜ ì „ì‚¬ + WER/CER í‰ê°€ (ë³µì§€ ì½œì„¼í„° ë°ì´í„° ì „ìš©)

ğŸ—‚ï¸  ê¸°ëŒ€ í´ë” êµ¬ì¡°  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
<BASE_DATA_DIR>/
 â””â”€ <dataset_type>/                     # 1.Training, 2.Validation â€¦
     â”œâ”€ ë¼ë²¨ë§ë°ì´í„°/
     â”‚   â””â”€ <category>/<condition>/<speaker_id>/<file_id>.json
     â””â”€ ì›ì²œë°ì´í„°/
         â””â”€ <category>/<condition>/<speaker_id>/<file_id>.wav
â€» ì˜ˆ)  .../1.Training/ë¼ë²¨ë§ë°ì´í„°/01.ëŒ€í•™ë³‘ì›/01.ì§„ë£Œì•ˆë‚´/01.ê²€ì‚¬/HOSâ€¦json
"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from pathlib import Path
from typing  import List
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import torch, librosa, jiwer, pandas as pd, json, re
from tqdm.auto import tqdm

CONFIG = dict(
    MODEL_ID      = "openai/whisper-tiny",
    DEVICE        = "cuda" if torch.cuda.is_available() else "cpu",
    BATCH_SIZE    = 3,                   # GPU ì‚¬ì–‘ì— ë§ê²Œ ì¡°ì •
    BASE_DATA_DIR = Path(r"D:/OneDrive/186.ë³µì§€ ë¶„ì•¼ ì½œì„¼í„° ìƒë‹´ë°ì´í„°/186.ë³µì§€ ë¶„ì•¼ ì½œì„¼í„° ìƒë‹´ë°ì´í„°/01.ë°ì´í„°"),
    DATASET_TYPES = ["1.Training"],      # í•„ìš”í•˜ë©´ "2.Validation" ì¶”ê°€
    TARGET_CATS   = ["01.ëŒ€í•™ë³‘ì›"],     # []ë©´ ëª¨ë“  ì¹´í…Œê³ ë¦¬
    TARGET_CONDS  = ["01.ê²€ì‚¬"],                  # []ë©´ ëª¨ë“  ì¡°ê±´
    SAVE_DIR      = Path(r"D:/OneDrive/186.ë³µì§€ ê²°ê³¼"),
    EXPECTED_LEN  = 3_000,               # Whisper mel-spectrogram ê¸¸ì´
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ëª¨ë¸ ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
proc  = WhisperProcessor.from_pretrained(CONFIG["MODEL_ID"])
model = WhisperForConditionalGeneration.from_pretrained(
            CONFIG["MODEL_ID"],
            torch_dtype=torch.float16
        ).to(CONFIG["DEVICE"]).eval()

decode_prompt = proc.get_decoder_prompt_ids(language="ko", task="transcribe")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ìœ í‹¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def pad_or_trim(feats, target: int):
    diff = target - feats.shape[-1]
    return (torch.nn.functional.pad(feats, (0, diff)) if diff > 0
            else feats[..., :target])

@torch.inference_mode()
def transcribe_batch(wavs: List[Path]) -> List[str]:
    waves = []
    for p in wavs:
        try:
            audio, _ = librosa.load(p, sr=16_000)
            waves.append(audio)
        except Exception as e:
            print(f"â— ì˜¤ë””ì˜¤ ë¡œë”© ì‹¤íŒ¨: {p} -> {e}")
            waves.append(None)

    valid_idx = [i for i, w in enumerate(waves) if w is not None]
    if not valid_idx:
        return [""] * len(wavs)

    feats = proc([waves[i] for i in valid_idx],
                 sampling_rate=16_000,
                 return_tensors="pt",
                 padding=True).input_features.half()
    feats = pad_or_trim(feats, CONFIG["EXPECTED_LEN"]).to(CONFIG["DEVICE"])

    ids   = model.generate(feats, num_beams=1, forced_decoder_ids=decode_prompt)
    texts = proc.batch_decode(ids, skip_special_tokens=True)

    out = [""] * len(wavs)
    for k, i in enumerate(valid_idx):
        out[i] = texts[k].strip()
    return out

def json_text(path: Path) -> str:
    """
    186 ë°ì´í„°ì…‹ JSON êµ¬ì¡°
        {
          "utterances": [
              { "text": "ì•ˆë…•í•˜ì„¸ìš”~", ... },
              ...
          ]
        }
    ë˜ëŠ” "inputText" í‚¤ê°€ ìˆì„ ìˆ˜ë„ ìˆì–´ ë‘˜ ë‹¤ ì²˜ë¦¬.
    """
    try:
        with path.open(encoding="utf-8") as f:
            data = json.load(f)

        if "utterances" in data:                      # 186 ë°ì´í„°
            texts = [u.get("text", "") for u in data["utterances"]]
        elif "inputText" in data:                     # ê¸°ì¡´ mentalhealth í¬ë§·
            texts = [t["orgtext"] for t in data["inputText"]]
        else:                                         # ì˜ˆì™¸ ì²˜ë¦¬
            texts = re.findall(r'"text"\s*:\s*"(.+?)"', json.dumps(data))

        return " ".join(texts).strip()

    except Exception as e:
        print(f"â— JSON íŒŒì‹± ì‹¤íŒ¨: {path} -> {e}")
        return ""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ë©”ì¸ ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for ds_type in CONFIG["DATASET_TYPES"]:
    base_lbl = CONFIG["BASE_DATA_DIR"] / ds_type / "ë¼ë²¨ë§ë°ì´í„°"
    base_org = CONFIG["BASE_DATA_DIR"] / ds_type / "ì›ì²œë°ì´í„°"

    for cat_dir in (base_lbl.iterdir() if not CONFIG["TARGET_CATS"]
                    else [base_lbl / c for c in CONFIG["TARGET_CATS"]]):
        if not cat_dir.exists(): continue
        cat = cat_dir.name

        # â”€â”€ 1ì°¨: ì¡°ê±´(ì˜ˆ: 01.ì§„ë£Œì•ˆë‚´) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for cond_dir in cat_dir.iterdir():
            if not cond_dir.is_dir(): continue
            cond1 = cond_dir.name
            if CONFIG["TARGET_CONDS"] and cond1 not in CONFIG["TARGET_CONDS"]:
                continue

            # â˜… 2ì°¨: **ì„¸ë¶€ ì¡°ê±´(ì˜ˆ: 01.ê²€ì‚¬, 02.ì…ì›â€¦) ì¶”ê°€ ë£¨í”„**
            for subcond_dir in cond_dir.iterdir():
                if not subcond_dir.is_dir(): continue
                cond2 = subcond_dir.name          # ì„¸ë¶€ ì¡°ê±´ ì´ë¦„
                cond_path = f"{cond1}/{cond2}"    # ë¡œê·¸Â·CSVìš© ì „ì²´ ì¡°ê±´ëª…

                print(f"\nğŸš€ [{ds_type}] {cat}/{cond_path}")
                records, wav_buf, json_buf, meta_buf = [], [], [], []

                # â”€â”€ 3ì°¨: speaker_id â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                for spk_dir in tqdm(list(subcond_dir.iterdir()),
                                    desc=cond_path, leave=False):
                    for jpath in spk_dir.glob("*.json"):
                        fid   = jpath.stem
                        wpath = (base_org / cat / cond1 / cond2 /
                                 spk_dir.name / f"{fid}.wav")
                        if not wpath.exists():
                            continue

                        wav_buf.append(wpath)
                        json_buf.append(jpath)
                        meta_buf.append((fid, spk_dir.name))

                        if len(wav_buf) == CONFIG["BATCH_SIZE"]:
                            preds = transcribe_batch(wav_buf)
                            for (fid_, spk_), jp, pr in zip(meta_buf,
                                                            json_buf, preds):
                                gt = json_text(jp)
                                records.append(dict(
                                    file_id=fid_, category=cat,
                                    condition=cond_path, speaker_id=spk_,
                                    gt_text=gt, pred_text=pr,
                                    WER=jiwer.wer(gt, pr) if gt else None,
                                    CER=jiwer.cer(gt, pr) if gt else None
                                ))
                            wav_buf, json_buf, meta_buf = [], [], []

                # â”€â”€ ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if wav_buf:
                    preds = transcribe_batch(wav_buf)
                    for (fid_, spk_), jp, pr in zip(meta_buf,
                                                    json_buf, preds):
                        gt = json_text(jp)
                        records.append(dict(
                            file_id=fid_, category=cat,
                            condition=cond_path, speaker_id=spk_,
                            gt_text=gt, pred_text=pr,
                            WER=jiwer.wer(gt, pr) if gt else None,
                            CER=jiwer.cer(gt, pr) if gt else None
                        ))

                if not records:
                    print("âš  ë°ì´í„° ì—†ìŒ â€” ê±´ë„ˆëœ€")
                    continue

                # â”€â”€ CSV ì €ì¥ (cond1 ë””ë ‰í„°ë¦¬ ì•ˆì— cond2.csv) â”€â”€â”€â”€â”€
                df = pd.DataFrame(records)
                df.loc[len(df)] = dict(file_id="AVG", category="",
                                       condition="", speaker_id="",
                                       gt_text="", pred_text="",
                                       WER=round(df["WER"].mean(skipna=True),4),
                                       CER=round(df["CER"].mean(skipna=True),4))

                save_path = (CONFIG["SAVE_DIR"] / ds_type / cat / cond1)
                save_path.mkdir(parents=True, exist_ok=True)
                out_csv = save_path / f"{cond2}.csv"
                df.to_csv(out_csv, index=False)
                print(f"âœ” {out_csv}  (ìƒ˜í”Œ {len(df)-1},  í‰ê·  "
                      f"WER={df['WER'].iloc[-1]}, CER={df['CER'].iloc[-1]})")